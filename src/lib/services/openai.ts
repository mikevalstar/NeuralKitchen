import OpenAI from "openai";
import { Prompts } from "../data/prompts";
import { Settings } from "../data/settings";

// Lazy-initialized OpenAI client
let openai: OpenAI | null = null;

function getOpenAIClient(): OpenAI {
  if (!openai) {
    const apiKey = Settings.get("OPENAI_API_KEY");
    if (!apiKey) {
      throw new Error("OPENAI_API_KEY setting is not configured");
    }
    openai = new OpenAI({ apiKey });
  }
  return openai;
}

export namespace OpenAIService {
  /**
   * Summarize a recipe document using GPT-4o-mini
   */
  export async function summarizeRecipe(title: string, content: string): Promise<string> {
    try {
      const client = getOpenAIClient();

      // Get prompts from database with fallback to constants
      const [systemPrompt, userPromptTemplate] = await Promise.all([
        Prompts.getByKey("RECIPE_SUMMARY_SYSTEM"),
        Prompts.getByKey("RECIPE_SUMMARY_USER"),
      ]);

      // Replace placeholders in user prompt
      const userPrompt = userPromptTemplate.replace("{title}", title).replace("{content}", content);

      const response = await client.chat.completions.create({
        model: "gpt-4o-mini",
        messages: [
          {
            role: "system",
            content: systemPrompt,
          },
          {
            role: "user",
            content: userPrompt,
          },
        ],
        max_tokens: 2000,
        temperature: 0.3,
      });

      const summary = response.choices[0]?.message?.content?.trim();

      if (!summary) {
        throw new Error("No summary generated by OpenAI");
      }

      return summary;
    } catch (error) {
      console.error("Error calling OpenAI API:", error);
      throw new Error(`Failed to summarize recipe: ${error instanceof Error ? error.message : "Unknown error"}`);
    }
  }

  /**
   * Simple token estimation - approximately 4 characters per token for English text
   * This is a rough approximation since proper tokenization requires the specific model's tokenizer
   */
  function estimateTokens(text: string): number {
    return Math.ceil(text.length / 4);
  }

  /**
   * Truncate text to approximately the specified number of tokens
   */
  function truncateToTokens(text: string, maxTokens: number): string {
    const estimatedTokens = estimateTokens(text);

    if (estimatedTokens <= maxTokens) {
      return text;
    }

    // Estimate how many characters we need to keep
    const targetLength = Math.floor(maxTokens * 4);

    // Truncate and try to break at a word boundary if possible
    let truncated = text.slice(0, targetLength);
    const lastSpaceIndex = truncated.lastIndexOf(" ");

    // If we can find a space within the last 100 characters, break there
    if (lastSpaceIndex > targetLength - 100) {
      truncated = truncated.slice(0, lastSpaceIndex);
    }

    return truncated;
  }

  /**
   * Generate embeddings for a recipe document (for future vector search)
   * Using text-embedding-3-small for cost efficiency
   */
  export async function generateEmbedding(text: string): Promise<number[]> {
    try {
      const client = getOpenAIClient();

      // Truncate text to approximately 6000 tokens to stay well within the 8192 token limit
      const truncatedText = truncateToTokens(text, 6000);

      const response = await client.embeddings.create({
        model: "text-embedding-3-small",
        input: truncatedText,
      });

      const embedding = response.data[0]?.embedding;

      if (!embedding) {
        throw new Error("No embedding generated by OpenAI");
      }

      return embedding;
    } catch (error) {
      console.error("Error generating embedding:", error);
      throw new Error(`Failed to generate embedding: ${error instanceof Error ? error.message : "Unknown error"}`);
    }
  }
}
